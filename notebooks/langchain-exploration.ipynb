{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path=\"/Users/dloader/Documents/GitHub/lazy-cv/references/Dylan-Loader-Resume-October-2023.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='\\tDylan Loader DylanLoader@gmail.com | (780) 293-1570 | Edmonton, AB | linkedin.com/in/Dylan-loader/  SKILLS  Data Science and Analytics: Applied various data extraction and cleaning approaches to real-world financial time-series data, including transaction and financial asset data Supervised Machine Learning: Designed logistic regression algorithms for classification and tree-based models for classification and regression Unsupervised Machine Learning: Designed Euclidean and Density-Based clustering algorithms as preprocessing work for downstream tasks Programming and Model Construction: Utilized Git platforms such as GitLab and GitHub to facilitate collaboration with team members for code tracking and documentation, Erwin Data Modeller Communication: Wrote documentation for project tracking and presented results to many internal and external stakeholders across various organizations  EDUCATION   University of Calgary                                         April 2024 MSc Statistics                                                                     Calgary, AB  University of Calgary                        April 2019 BSc Statistics, Minor in Data Science                                                                           Calgary, AB  MacEwan University                                                                    April 2016 BSc Mathematics, Minor in Economics                                           Edmonton, AB  WORK EXPERIENCE   Data Science Consultant (Internship)                        October 2022 – July 2023 ATB Financial                             Calgary, AB • Querying and transforming data through Google BigQuery queries with SQL • Researching methods for implementing Graph Feature Engineering in fraud detection pipelines • Developing a knowledge base with the CRISP-DM framework to document the project requirements and progress • Implementing Gradient Boosting Models with Catboost and XGBoost to validate engineered features against currently implemented feature sets on transaction datasets in the 10M-100M observation scale  Associate Machine Learning Developer (Internship)                                               July 2021 - September 2021 AltaML and ATB Financial                Calgary, AB • Built pre-processing pipelines for down-stream internal projects at ATB • Experimented with Cluster Analysis to improve customer segmentation implementations • Presented weekly experimentation progress and provided evidence-backed recommendations to senior managers and directors  Data Science Development (Internship)               June 2020 - March 2021 Benevity Inc.                  Calgary, AB • Built scalable data ingestion and cleaning pipelines in Python through AWS SageMaker • Proposed and implemented data imputation methods to improve data completeness • Researched and documented anomaly detection methods to augment internal fraud investigations  Graduate Teaching Assistant (Data Science)         September 2019 - December 2019 University of Calgary                                       Calgary, AB • Clarified lecture material and learning outcomes during and after tutorials through in person instruction and email • Assisted students through code reviews in R and report writing in RMarkdown • Guided students through project design, improvement, and implementation for final course projects  ', metadata={'source': '/Users/dloader/Documents/GitHub/lazy-cv/references/Dylan-Loader-Resume-October-2023.pdf', 'page': 0})]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk = RecursiveCharacterTextSplitter(chunk_size=10000, \n",
    "                                            chunk_overlap=20).split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Dylan Loader DylanLoader@gmail.com | (780) 293-1570 | Edmonton, AB | linkedin.com/in/Dylan-loader/  SKILLS  Data Science and Analytics: Applied various data extraction and cleaning approaches to real-world financial time-series data, including transaction and financial asset data Supervised Machine Learning: Designed logistic regression algorithms for classification and tree-based models for classification and regression Unsupervised Machine Learning: Designed Euclidean and Density-Based clustering algorithms as preprocessing work for downstream tasks Programming and Model Construction: Utilized Git platforms such as GitLab and GitHub to facilitate collaboration with team members for code tracking and documentation, Erwin Data Modeller Communication: Wrote documentation for project tracking and presented results to many internal and external stakeholders across various organizations  EDUCATION   University of Calgary                                         April 2024 MSc Statistics                                                                     Calgary, AB  University of Calgary                        April 2019 BSc Statistics, Minor in Data Science                                                                           Calgary, AB  MacEwan University                                                                    April 2016 BSc Mathematics, Minor in Economics                                           Edmonton, AB  WORK EXPERIENCE   Data Science Consultant (Internship)                        October 2022 – July 2023 ATB Financial                             Calgary, AB • Querying and transforming data through Google BigQuery queries with SQL • Researching methods for implementing Graph Feature Engineering in fraud detection pipelines • Developing a knowledge base with the CRISP-DM framework to document the project requirements and progress • Implementing Gradient Boosting Models with Catboost and XGBoost to validate engineered features against currently implemented feature sets on transaction datasets in the 10M-100M observation scale  Associate Machine Learning Developer (Internship)                                               July 2021 - September 2021 AltaML and ATB Financial                Calgary, AB • Built pre-processing pipelines for down-stream internal projects at ATB • Experimented with Cluster Analysis to improve customer segmentation implementations • Presented weekly experimentation progress and provided evidence-backed recommendations to senior managers and directors  Data Science Development (Internship)               June 2020 - March 2021 Benevity Inc.                  Calgary, AB • Built scalable data ingestion and cleaning pipelines in Python through AWS SageMaker • Proposed and implemented data imputation methods to improve data completeness • Researched and documented anomaly detection methods to augment internal fraud investigations  Graduate Teaching Assistant (Data Science)         September 2019 - December 2019 University of Calgary                                       Calgary, AB • Clarified lecture material and learning outcomes during and after tutorials through in person instruction and email • Assisted students through code reviews in R and report writing in RMarkdown • Guided students through project design, improvement, and implementation for final course projects', metadata={'source': '/Users/dloader/Documents/GitHub/lazy-cv/references/Dylan-Loader-Resume-October-2023.pdf', 'page': 0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the sentence transformer emb\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding store\n",
    "vector_store = FAISS.from_documents(text_chunk, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.faiss.FAISS at 0x2d0ac5290>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../ollama/text-generation-webui/models/mistral-11b-omnimix-bf16.Q5_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 435 tensors from ../ollama/text-generation-webui/models/mistral-11b-omnimix-bf16.Q5_K_M.gguf (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q5_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:           blk.26.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.26.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:             blk.26.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:             blk.26.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:        blk.26.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.26.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.26.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:           blk.27.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.27.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.27.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.27.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:        blk.27.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.27.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:             blk.27.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:           blk.28.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.28.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.28.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.28.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:        blk.28.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.28.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.28.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:           blk.29.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.29.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.29.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.29.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:        blk.29.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.29.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:             blk.29.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:           blk.30.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.30.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.30.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.30.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:        blk.30.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.30.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:             blk.30.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:           blk.31.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.31.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:             blk.31.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:             blk.31.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:        blk.31.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.31.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:             blk.31.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:          blk.32.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:           blk.32.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.32.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.32.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.32.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:             blk.32.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:        blk.32.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.32.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.32.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:          blk.33.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:           blk.33.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.33.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.33.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.33.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:             blk.33.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:        blk.33.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.33.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:             blk.33.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:          blk.34.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:           blk.34.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.34.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.34.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.34.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:             blk.34.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:        blk.34.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.34.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.34.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:          blk.35.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:           blk.35.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.35.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.35.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.35.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:             blk.35.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:        blk.35.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.35.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.35.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:            blk.2.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:              blk.2.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:              blk.2.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:         blk.2.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.2.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:              blk.2.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:           blk.20.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:           blk.20.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.20.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:             blk.20.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:        blk.20.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.20.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.20.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.21.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.21.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.21.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.21.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.21.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.21.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.21.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.22.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.22.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.22.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.22.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.22.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.22.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.22.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.23.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.23.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.23.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.23.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.23.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.23.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.23.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.24.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.24.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.24.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.24.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.24.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.24.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.24.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.25.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.25.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.25.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.25.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.25.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.25.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.25.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:            blk.3.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:            blk.3.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:              blk.3.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:              blk.3.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:         blk.3.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:              blk.3.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:              blk.3.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.36.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.36.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.36.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.36.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.36.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.36.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.36.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.36.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.36.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.37.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.37.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.37.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.37.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.37.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.37.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.37.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.37.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.37.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:          blk.38.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:           blk.38.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:           blk.38.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.38.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:           blk.38.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:             blk.38.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:        blk.38.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.38.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:             blk.38.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:          blk.39.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:           blk.39.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:           blk.39.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.39.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:           blk.39.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:             blk.39.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:        blk.39.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.39.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:             blk.39.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:            blk.4.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:            blk.4.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:              blk.4.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:              blk.4.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:         blk.4.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:              blk.4.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:              blk.4.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:          blk.40.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:           blk.40.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:           blk.40.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.40.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:           blk.40.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:             blk.40.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:        blk.40.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.40.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:             blk.40.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:          blk.41.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:           blk.41.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:           blk.41.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.41.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:           blk.41.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:             blk.41.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:        blk.41.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.41.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:             blk.41.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:          blk.42.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:           blk.42.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:           blk.42.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.42.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:           blk.42.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:             blk.42.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:        blk.42.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.42.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:             blk.42.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:          blk.43.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:           blk.43.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:           blk.43.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.43.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:           blk.43.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:             blk.43.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:        blk.43.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.43.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:             blk.43.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:          blk.44.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:           blk.44.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:           blk.44.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:            blk.5.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:            blk.5.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:              blk.5.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:              blk.5.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:         blk.5.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:              blk.5.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:              blk.5.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:            blk.6.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:            blk.6.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:              blk.6.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:              blk.6.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:         blk.6.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:              blk.6.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:              blk.6.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:            blk.7.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:            blk.7.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:              blk.7.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:              blk.7.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:         blk.7.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:              blk.7.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:              blk.7.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:            blk.8.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:            blk.8.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:              blk.8.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:              blk.8.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:         blk.8.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:              blk.8.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:              blk.8.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:            blk.9.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:            blk.9.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:              blk.9.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:              blk.9.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:         blk.9.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:              blk.9.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:              blk.9.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:           blk.44.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:             blk.44.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:        blk.44.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:             blk.44.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:          blk.45.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:           blk.45.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:           blk.45.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:           blk.45.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:             blk.45.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:        blk.45.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:             blk.45.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:          blk.46.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:           blk.46.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:           blk.46.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:           blk.46.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:             blk.46.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:        blk.46.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:             blk.46.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:          blk.47.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:           blk.47.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:           blk.47.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:           blk.47.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:             blk.47.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:        blk.47.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:             blk.47.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32     \n",
      "llama_model_loader: - kv  11:                          general.file_type u32     \n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  19:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   97 tensors\n",
      "llama_model_loader: - type q5_K:  289 tensors\n",
      "llama_model_loader: - type q6_K:   49 tensors\n",
      "llm_load_print_meta: format           = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 48\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: model type       = 34B\n",
      "llm_load_print_meta: model ftype      = mostly Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 10.73 B\n",
      "llm_load_print_meta: model size       = 7.08 GiB (5.66 BPW) \n",
      "llm_load_print_meta: general.name   = neversleep_mistral-11b-omnimix-bf16\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MB\n",
      "llm_load_tensors: mem required  = 7245.38 MB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  768.00 MB\n",
      "llama_new_context_with_model: compute buffer total size = 10.38 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# import model \n",
    "llm = LlamaCpp(\n",
    "    streaming=True, \n",
    "    model_path=model_path,\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    n_ctx=4096,\n",
    "    verbose=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import document_loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever(search_kwargs={\"k\": 2}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  3954.02 ms\n",
      "llama_print_timings:      sample time =    58.48 ms /    87 runs   (    0.67 ms per token,  1487.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  7800.63 ms /    87 runs   (   89.66 ms per token,    11.15 tokens per second)\n",
      "llama_print_timings:       total time =  7960.48 ms\n"
     ]
    }
   ],
   "source": [
    "query = \"Based on my last jobs what are 3 jobs I should apply for next?\"\n",
    "returned_q = qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are some job titles you may want to consider based on your experience and skills:\n",
      "1. Data Scientist\n",
      "2. Machine Learning Engineer\n",
      "3. Data Analyst\n",
      "\n",
      "These roles align well with your background in data science, programming, machine learning, and analytics, as well as your experience using statistical software such as R and Python. Additionally, your project management and communication skills would be valuable in these positions.\n"
     ]
    }
   ],
   "source": [
    "print(returned_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_cover_letter:\n",
    "    # Prompts\n",
    "    pre_prompt = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n",
    "    # Create a prompt for LLM: Include user inputs, and job description in the prompt\n",
    "    prompt = f\"The job description is: {prompt_input}\\n\"\n",
    "    prompt += f\"The candidate's name to include on the cover letter: {user_name}\\n\"\n",
    "    prompt += f\"The job title/role: {role}\\n\"\n",
    "    prompt += f\"The hiring manager is: {manager}\\n\"\n",
    "    prompt += f\"How I heard about the opportunity: {referral}\\n.\"\n",
    "    prompt += \"Generate a cover letter\"\n",
    "    # Generate LLM response\n",
    "    with st.spinner(\"Generating response\"):\n",
    "        response = replicate.run(\n",
    "            'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5',  # Llama 2 model\n",
    "            input={\n",
    "                \"prompt\": f\"{pre_prompt} {prompt} Assistant:\",\n",
    "                \"temperature\": temp,\n",
    "            }\n",
    "        )\n",
    "        # Extract and display the LLM-generated cover letter\n",
    "        generated_cover_letter = \" \".join([item for item in response])\n",
    "    \n",
    "    st.subheader(\"Generated Cover Letter:\")\n",
    "    st.write(generated_cover_letter)\n",
    "    # Offer a download link for the generated cover letter\n",
    "    st.subheader(\"Download Generated Cover Letter:\")\n",
    "    st.download_button(\"Download Cover Letter as TXT\", generated_cover_letter, key=\"cover_letter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lazy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
